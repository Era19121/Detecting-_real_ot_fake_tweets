{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_Extraction",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeIpeGhfUTwv",
        "colab_type": "text"
      },
      "source": [
        "**Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnScfWieKQwy",
        "colab_type": "code",
        "outputId": "c1e0dbe2-db7e-4542-b0e6-b9466489b059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet_ic')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "from nltk.corpus import wordnet_ic\n",
        "traindata = pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "testdata = pd.read_csv('/content/drive/My Drive/test.csv')\n",
        "print(traindata.columns)\n",
        "print(testdata.columns)\n",
        "ids = list(traindata['id'])\n",
        "target_train=list(traindata['target'])\n",
        "train_text = traindata['text']\n",
        "test_text = testdata['text']\n",
        "co_ex_tr=[]\n",
        "co_qe_tr=[]\n",
        "co_h_tr=[]\n",
        "co_word=[]\n",
        "co_letters=[]\n",
        "co_url_tr=[]\n",
        "co_at_tr=[]\n",
        "co_upper_tr=[]\n",
        "for i in train_text:\n",
        "    co_ex_tr.append(i.count('!'))\n",
        "    co_qe_tr.append(i.count('?'))\n",
        "    co_h_tr.append(i.count('#'))\n",
        "    co_letters.append(len(i))\n",
        "    w = i.split(' ')\n",
        "    co_word.append(len(w))\n",
        "    co_url_tr.append(i.count('http:'))\n",
        "    co_at_tr.append(i.count('@'))\n",
        "    c=0\n",
        "    for j in i:\n",
        "        if(j.isupper()):\n",
        "            c=c+1\n",
        "    co_upper_tr.append(c)\n",
        "co_ex_te=[]\n",
        "co_qe_te=[]\n",
        "co_h_te=[]\n",
        "co_word_te=[]\n",
        "co_letters_te=[]\n",
        "co_url_te=[]\n",
        "co_upper_te=[]\n",
        "co_at_te=[]\n",
        "for i in test_text:\n",
        "    co_ex_te.append(i.count('!'))\n",
        "    co_qe_te.append(i.count('?'))\n",
        "    co_h_te.append(i.count('#'))\n",
        "    co_letters_te.append(len(i))\n",
        "    co_url_te.append(i.count('http:'))\n",
        "    co_at_te.append(i.count('@'))\n",
        "    w = i.split(' ')\n",
        "    co_word_te.append(len(w))\n",
        "    c=0\n",
        "    for j in i:\n",
        "        if(j.isupper()):\n",
        "            c=c+1\n",
        "    co_upper_te.append(c)\n",
        "        \n",
        "#traindata['keyword'].fillna('blank')\n",
        "#testdata['keyword'].fillna('blank')\n",
        "#traindata['location'].fillna('blank')\n",
        "#testdata['location'].fillna('blank')\n",
        "traindata = traindata.drop(columns=['keyword','location','id','target','text'],axis=0)\n",
        "testdata = testdata.drop(columns=['keyword','location','id','text'],axis=0)\n",
        "traindata['Count of ex_mark'] = co_ex_tr\n",
        "traindata['Count_or_question_mark'] = co_qe_tr\n",
        "traindata['Count_of_#'] = co_h_tr\n",
        "traindata['Length in characters'] = co_letters\n",
        "traindata['Length in words'] = co_word\n",
        "traindata['Count_of_URL'] = co_url_tr\n",
        "traindata['Count_of_@'] = co_at_tr\n",
        "testdata['Count of ex_mark'] = co_ex_te\n",
        "testdata['Count_or_question_mark'] = co_qe_te\n",
        "testdata['Count_of_#'] = co_h_te\n",
        "testdata['Length in characters'] = co_letters_te\n",
        "testdata['Length in words'] = co_word_te\n",
        "testdata['Count_of_URL'] = co_url_te\n",
        "testdata['Count_of_@'] = co_at_te\n",
        "testdata['Count_of_uppercase'] = co_upper_te\n",
        "traindata['Count_of_uppercase'] = co_upper_tr\n",
        "\n",
        "\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "ps=PorterStemmer()\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "data = pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/My Drive/test.csv')\n",
        "real_word=[]\n",
        "nonreal_word=[]\n",
        "for i in range(5000):\n",
        "    if data['target'][i]==0:\n",
        "        tokens = nltk.word_tokenize(data['text'][i])\n",
        "        tokens = [w.lower() for w in tokens]\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [w.translate(table) for w in tokens]\n",
        "        words = [word for word in stripped if word.isalpha()]\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [w for w in words if not w in stop_words]\n",
        "        lmt = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
        "        lm = [ps.stem(word) for word in lmt]\n",
        "        for l in lm:\n",
        "            if l not in nonreal_word:\n",
        "                if wn.synsets(l,pos='n'):\n",
        "                    nonreal_word.append(l)\n",
        "    else:\n",
        "        tokens = nltk.word_tokenize(data['text'][i])\n",
        "        tokens = [w.lower() for w in tokens]\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        stripped = [w.translate(table) for w in tokens]\n",
        "        words = [word for word in stripped if word.isalpha()]\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [w for w in words if not w in stop_words]\n",
        "        lmt = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
        "        lm = [ps.stem(word) for word in lmt]\n",
        "        for l in lm:\n",
        "            if l not in real_word:\n",
        "                if wn.synsets(l,pos='n'):\n",
        "                    real_word.append(l)\n",
        "        \n",
        "original =[]\n",
        "computed=[]\n",
        "real_similarity=[]\n",
        "fake_similarity=[]\n",
        "for lk in range(len(data['target'])):\n",
        "    word=[]\n",
        "    tokens = nltk.word_tokenize(data['text'][lk])\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    lmt = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
        "    lm = [ps.stem(word) for word in lmt]\n",
        "    simi1=[]\n",
        "    simi2=[]\n",
        "    for l in lm:\n",
        "        if l not in word:\n",
        "            if wn.synsets(l,pos='n'):\n",
        "                word.append(l)\n",
        "   \n",
        "    if len(word)==0:\n",
        "        real_similarity.append(0)\n",
        "        fake_similarity.append(0)\n",
        "        continue\n",
        "    for i in word:\n",
        "        try:\n",
        "            s = wn.synset(i+'.n.01')\n",
        "        except:\n",
        "            s = None\n",
        "            continue\n",
        "        for k in real_word:\n",
        "            try:\n",
        "                s2 = wn.synset(k+'.n.01')\n",
        "            except:\n",
        "                s2=None\n",
        "                continue\n",
        "            \n",
        "            sim = s.res_similarity(s2,brown_ic)\n",
        "            simi1.append(sim)\n",
        "    #calculate resnik similarity between the synonym or the sense of each word of question 2 with\n",
        "    #sense of each word of question 1 \n",
        "    for i in real_word:\n",
        "        try:\n",
        "            s = wn.synset(i+'.n.01')\n",
        "        except:\n",
        "            s = None\n",
        "            continue\n",
        "        for k in word:\n",
        "            try:\n",
        "                s2 = wn.synset(k+'.n.01')\n",
        "            except:\n",
        "                s2=None\n",
        "                continue\n",
        "            sim = s.res_similarity(s2,brown_ic)\n",
        "            simi2.append(sim)\n",
        "    #find the average of the similarities of two words which have the maximum similarity\n",
        "    if(len(simi1) == 0 and len(simi2)==0):\n",
        "        similarity1=0\n",
        "    elif(len(simi1) == 0):\n",
        "        similarity1 = (max(simi2))/2\n",
        "    elif(len(simi2)==0):\n",
        "        similarity1 = (max(simi1))/2\n",
        "    else:\n",
        "        similarity1 = (max(simi1)+max(simi2))/2\n",
        "    real_similarity.append(similarity1)\n",
        "    simi1=[]\n",
        "    simi2=[]\n",
        "    for i in word:\n",
        "        try:\n",
        "            s = wn.synset(i+'.n.01')\n",
        "        except:\n",
        "            s = None\n",
        "            continue\n",
        "        for k in nonreal_word:\n",
        "            try:\n",
        "                s2 = wn.synset(k+'.n.01')\n",
        "            except:\n",
        "                s2=None\n",
        "                continue\n",
        "            sim = s.res_similarity(s2,brown_ic)\n",
        "            simi1.append(sim)\n",
        "    #calculate resnik similarity between the synonym or the sense of each word of question 2 with\n",
        "    #sense of each word of question 1 \n",
        "    for i in nonreal_word:\n",
        "        try:\n",
        "            s = wn.synset(i+'.n.01')\n",
        "        except:\n",
        "            s = None\n",
        "            continue\n",
        "        for k in word:\n",
        "            try:\n",
        "                s2 = wn.synset(k+'.n.01')\n",
        "            except:\n",
        "                s2=None\n",
        "                continue\n",
        "            sim = s.res_similarity(s2,brown_ic)\n",
        "            simi2.append(sim)\n",
        "    #find the average of the similarities of two words which have the maximum similarity\n",
        "    if(len(simi1) == 0 and len(simi2)==0):\n",
        "        similarity2=0\n",
        "    elif(len(simi1) == 0):\n",
        "        similarity2 = (max(simi2))/2\n",
        "    elif(len(simi2)==0):\n",
        "        similarity2 = (max(simi1))/2\n",
        "    else:\n",
        "        similarity2 = (max(simi1)+max(simi2))/2\n",
        "    fake_similarity.append(similarity2)\n",
        "traindata['Resnik_Similarity_real'] = real_similarity\n",
        "traindata['Resnik_Similarity_fake'] = fake_similarity\n",
        "real_similarity_te=[]\n",
        "fake_similarity_te=[]\n",
        "for lk in range(len(test_data['text'])):\n",
        "    word=[]\n",
        "    tokens = nltk.word_tokenize(test_data['text'][lk])\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    lmt = [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
        "    lm = [ps.stem(word) for word in lmt]\n",
        "    simi1=[]\n",
        "    simi2=[]\n",
        "    for l in lm:\n",
        "        if l not in word:\n",
        "            if wn.synsets(l,pos='n'):\n",
        "                word.append(l)\n",
        "   \n",
        "    if len(word)==0:\n",
        "        real_similarity_te.append(0)\n",
        "        fake_similarity_te.append(0)\n",
        "        continue\n",
        "    for i in word:\n",
        "        try:\n",
        "            s = wn.synset(i+'.n.01')\n",
        "        except:\n",
        "            s = None\n",
        "            continue\n",
        "        for k in real_word:\n",
        "            try:\n",
        "                s2 = wn.synset(k+'.n.01')\n",
        "            except:\n",
        "                s2=None\n",
        "                continue\n",
        "            \n",
        "            sim = s.res_similarity(s2,brown_ic)\n",
        "            simi1.append(sim)\n",
        "    #calculate resnik similarity between the synonym or the sense of each word of question 2 with\n",
        "    #sense of each word of question 1 \n",
        "    for i in real_word:\n",
        "        try:\n",
        "            s = wn.synset(i+'.n.01')\n",
        "        except:\n",
        "            s = None\n",
        "            continue\n",
        "        for k in word:\n",
        "            try:\n",
        "                s2 = wn.synset(k+'.n.01')\n",
        "            except:\n",
        "                s2=None\n",
        "                continue\n",
        "            sim = s.res_similarity(s2,brown_ic)\n",
        "            simi2.append(sim)\n",
        "    #find the average of the similarities of two words which have the maximum similarity\n",
        "    if(len(simi1) == 0 and len(simi2)==0):\n",
        "        similarity1=0\n",
        "    elif(len(simi1) == 0):\n",
        "        similarity1 = (max(simi2))/2\n",
        "    elif(len(simi2)==0):\n",
        "        similarity1 = (max(simi1))/2\n",
        "    else:\n",
        "        similarity1 = (max(simi1)+max(simi2))/2\n",
        "    real_similarity_te.append(similarity1)\n",
        "    simi1=[]\n",
        "    simi2=[]\n",
        "    for i in word:\n",
        "        try:\n",
        "            s = wn.synset(i+'.n.01')\n",
        "        except:\n",
        "            s = None\n",
        "            continue\n",
        "        for k in nonreal_word:\n",
        "            try:\n",
        "                s2 = wn.synset(k+'.n.01')\n",
        "            except:\n",
        "                s2=None\n",
        "                continue\n",
        "            sim = s.res_similarity(s2,brown_ic)\n",
        "            simi1.append(sim)\n",
        "    #calculate resnik similarity between the synonym or the sense of each word of question 2 with\n",
        "    #sense of each word of question 1 \n",
        "    for i in nonreal_word:\n",
        "        try:\n",
        "            s = wn.synset(i+'.n.01')\n",
        "        except:\n",
        "            s = None\n",
        "            continue\n",
        "        for k in word:\n",
        "            try:\n",
        "                s2 = wn.synset(k+'.n.01')\n",
        "            except:\n",
        "                s2=None\n",
        "                continue\n",
        "            sim = s.res_similarity(s2,brown_ic)\n",
        "            simi2.append(sim)\n",
        "    #find the average of the similarities of two words which have the maximum similarity\n",
        "    if(len(simi1) == 0 and len(simi2)==0):\n",
        "        similarity2=0\n",
        "    elif(len(simi1) == 0):\n",
        "        similarity2 = (max(simi2))/2\n",
        "    elif(len(simi2)==0):\n",
        "        similarity2 = (max(simi1))/2\n",
        "    else:\n",
        "        similarity2 = (max(simi1)+max(simi2))/2\n",
        "    fake_similarity_te.append(similarity2)\n",
        "testdata['Resnik_Similarity_real'] = real_similarity_te\n",
        "testdata['Resnik_Similarity_fake'] = fake_similarity_te\n",
        "traindata.to_csv(\"train_data.csv\",index=false)\n",
        "testdata.to_csv(\"test_data.csv\",index=false)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
            "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n",
            "Index(['id', 'keyword', 'location', 'text'], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3d4aa5985d66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Resnik_Similarity_real'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_similarity_te\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Resnik_Similarity_fake'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_similarity_te\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m \u001b[0mtraindata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'false' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bXD9-E8y-SQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traindata.to_csv('/content/drive/My Drive/Train_data.csv',index=False)\n",
        "testdata.to_csv('/content/drive/My Drive/Test_data.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6FU7LlczLIU",
        "colab_type": "code",
        "outputId": "b6803ea6-a50f-48ee-ee86-fb3a43974272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "new_data_tr = pd.read_csv('/content/drive/My Drive/Train_data_New.csv')\n",
        "new_data_te = pd.read_csv('/content/drive/My Drive/Test_data_New.csv')\n",
        "traindata = pd.read_csv()\n",
        "for i in traindata.columns:\n",
        "  if i in new_data_tr.columns:\n",
        "    continue\n",
        "  else:\n",
        "    new_data_tr[i] = traindata[i]\n",
        "for i in testdata.columns:\n",
        "  if i in new_data_te.columns:\n",
        "    continue\n",
        "  else:\n",
        "    new_data_te[i] = testdata[i]\n",
        "print(new_data_tr)\n",
        "print(new_data_te)\n",
        "new_data_te.to_csv('/content/drive/My Drive/Train_Data.csv',index=False)\n",
        "new_data_tr.to_csv('/content/drive/My Drive/Test_Data.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         id keyword  ... Resnik_Similarity_real Resnik_Similarity_fake\n",
            "0         1     NaN  ...           1.446560e+01           1.446560e+01\n",
            "1         4     NaN  ...           1.091025e+01           1.091025e+01\n",
            "2         5     NaN  ...          1.000000e+300           9.840627e+00\n",
            "3         6     NaN  ...           1.021710e+01           1.021710e+01\n",
            "4         7     NaN  ...           1.137456e+01           1.137456e+01\n",
            "...     ...     ...  ...                    ...                    ...\n",
            "7608  10869     NaN  ...          1.000000e+300          1.000000e+300\n",
            "7609  10870     NaN  ...           1.068141e+01           1.040516e+01\n",
            "7610  10871     NaN  ...          1.000000e+300          1.000000e+300\n",
            "7611  10872     NaN  ...           1.047662e+01           1.047662e+01\n",
            "7612  10873     NaN  ...          1.000000e+300          1.000000e+300\n",
            "\n",
            "[7613 rows x 19 columns]\n",
            "         id keyword  ... Resnik_Similarity_real Resnik_Similarity_fake\n",
            "0         0     NaN  ...           1.113340e+01           1.113340e+01\n",
            "1         2     NaN  ...           1.038806e+01           1.038806e+01\n",
            "2         3     NaN  ...           1.285616e+01           1.285616e+01\n",
            "3         9     NaN  ...           7.782239e+00           7.782239e+00\n",
            "4        11     NaN  ...           1.446560e+01           1.182654e+01\n",
            "...     ...     ...  ...                    ...                    ...\n",
            "3258  10861     NaN  ...           9.999692e+00           9.999692e+00\n",
            "3259  10865     NaN  ...           1.285616e+01           1.285616e+01\n",
            "3260  10868     NaN  ...          1.000000e+300          1.000000e+300\n",
            "3261  10874     NaN  ...          1.000000e+300          1.000000e+300\n",
            "3262  10875     NaN  ...           6.171550e+00           6.171550e+00\n",
            "\n",
            "[3263 rows x 18 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}