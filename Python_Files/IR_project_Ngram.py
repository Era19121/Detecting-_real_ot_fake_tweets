# -*- coding: utf-8 -*-
"""Copy_of_IR_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qwP9iSWG6-C2Daz98TZ9dVzJ3LZeadmy
"""

import numpy as np 
import pandas as pd 
import re
import nltk
from matplotlib import pyplot as plt
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt 
from wordcloud import WordCloud, STOPWORDS 
nltk.download("stopwords")
#stop = stopwords.words('english')

from google.colab import files
uploaded = files.upload()

#Reading input file.
import io
import pandas as pd
train = pd.read_csv(io.BytesIO(uploaded['train.csv']))

#test=pd.read_csv(io.BytesIO(uploaded['test.csv']))

train.head()

#Dropping useless features
train=train.drop(['id','keyword','location'], axis = 1) 
train

#Split the train data set to train and test data
X_train , X_test , y_train , y_test = train_test_split(train['text'],train['target'],test_size=0.2)

X_train

y_train

# Creating Corpus after preprocessing the training data
corpus  = []
pstem = PorterStemmer()
for i in (X_train):
    text = re.sub("[^a-zA-Z]", ' ', i)
    text = text.lower()
    text = text.split()
    text = [pstem.stem(word) for word in text if not word in set(stopwords.words('english'))]
    text = ' '.join(text)
    corpus.append(text)

(corpus)

#corpus for real and fake tweets
corpusfake  = []
corpusreal=[]
#tokenizer = RegexpTokenizer(r'[\w]+[-\']?[\w]+')
pstem = PorterStemmer()
for i in (X_train.keys()):
  #print(i)
  #print(y_train[i])
  #print(X_train[i])
  #print(y_train[i])
  if y_train[i]==0:
    #print((X_train[i]))
    text = re.sub("[^a-zA-Z]", ' ', X_train[i])
    text = text.lower()
    text = text.split()
    text = [pstem.stem(word) for word in text if not word in set(stopwords.words('english'))]
    text = ' '.join(text)
    corpusfake.append(text)
  else:
    print((X_train[i]))
    text = re.sub("[^a-zA-Z]", ' ', X_train[i])
    text = text.lower()
    text = text.split()
    text = [pstem.stem(word) for word in text if not word in set(stopwords.words('english'))]
    text = ' '.join(text)
    corpusreal.append(text)

print((corpusreal))

print(len(corpusfake))

len(X_train)

print(len(corpus))

#count of thw words in real  corpus

uniqueWords_real = {}
for text in corpusreal:
    #print(text)
    for word in text.split():
        #print(word)
        if(word in uniqueWords_real.keys()):
            uniqueWords_real[word] = uniqueWords_real[word] + 1
        else:
            uniqueWords_real[word] = 1

print(uniqueWords_real)

#count of thw words in fake corpus
uniqueWords_fake = {}
for text in corpusfake:
    #print(text)
    for word in text.split():
        #print(word)
        if(word in uniqueWords_fake.keys()):
            uniqueWords_fake[word] = uniqueWords_fake[word] + 1
        else:
            uniqueWords_fake[word] = 1

print(uniqueWords_fake)

#bigram and its count for real corpus
from collections import Counter
bigramreal = Counter()
for line in corpusreal:
  s = line.split()
  bigramreal.update('%s %s'%t for t in zip(s,s[1:]))

print(bigramreal)

#bigram and its count for fake corpus
from collections import Counter
bigramfake = Counter()
for line in corpusfake:
  s = line.split()
  bigramfake.update('%s %s'%t for t in zip(s,s[1:]))

print(bigramfake)

bigramreal['suicid bomber']

#Creating corpus of test data
#test.drop(['id','keyword','location'], axis = 1)
corpustest  = []
pstem = PorterStemmer()
for i in (X_test):
    text = re.sub("[^a-zA-Z]", ' ', i)
    text = text.lower()
    text = text.split()
    text = [pstem.stem(word) for word in text if not word in set(stopwords.words('english'))]
    text = ' '.join(text)
    corpustest.append(text)

corpustest

len(uniqueWords_fake)

#Functions used to determine bigram probabilities for real and fake corpus
def realprob(a,word):
  x=0
  #print(a[0])
  if a in bigramreal and word in uniqueWords_real:
    b=(bigramreal[a]+1)/(uniqueWords_real[word]+len(uniqueWords_real))
    x=b
  elif a not in bigramreal and word in uniqueWords_real:
    x=1/(uniqueWords_real[word]+len(uniqueWords_real))
  else:
    x=1/len(uniqueWords_real)
  #print('real',x)
  return (x)
def fakeprob(a,word):
  y=0
  if a in bigramfake and word in uniqueWords_fake:
    c=(bigramfake[a]+1)/(uniqueWords_fake[word]+len(uniqueWords_fake))
    y=c
  
  elif a not in bigramfake and word in uniqueWords_fake:
    y=1/(uniqueWords_fake[word]+len(uniqueWords_fake))
  else:
    y=1/len(uniqueWords_fake)
  #print('fake',y)
  return (y)

realprob('may broken','may')

# Calculating bigram probabilities for real and fake corpus and predicting labels of test data.
import math
label=[]
for i in corpustest:
  #print(i)
  res = i.split() 
  preal=1
  pfake=1
  #print(res)
  for i in range(len(res)-1):
    real=0
    fake=0
   # print(a)
    #print(word)
    #print(i)
    word=res[i]
    a= res[i]+" "+res[i+1]
    
    real=realprob(a,word)
    #print('hi',real,realprob(a,word))
    preal=preal*real
    fake=fakeprob(a,word)
    pfake=pfake*fake
 
  print(preal,pfake)
  if preal>pfake: 
    label.append(1)
  else:
    label.append(0)

print(label)

y_test

#Calculating accuracy.
i=0
count=0
for j in (y_test):
  #print(i)
  #print(i)
  #print(j)
  #print(j)
  if label[i] == j:
    count=count+1
  i=i+1
print(count)

print(len(y_test))

accuracy=count/len(y_test)

print(accuracy)

