# -*- coding: utf-8 -*-
"""IR_project_LR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jusOBWjuzi0fwqMv7loY9JOpa6KlzO2q
"""

import numpy as np
import pandas as pd
import nltk
import re
import string
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords 
from collections import defaultdict
from num2words import num2words
from nltk.tokenize import word_tokenize 
import collections
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import seaborn
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
import math
import json
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.decomposition import TruncatedSVD

import io
import pandas as pd
#reading train and text.csv and converting to dataframe
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
train_data

#extracted colums 
train_data.columns

#droppinf "id" column
train_data.drop(train_data.columns[[0]], axis=1, inplace=True)
train_data.drop(train_data.columns[[0]], axis=1, inplace=True)
'''
train_data.location.fillna("unknown",inplace=True)
train_data.keyword.fillna("none",inplace=True)
test_data.location.fillna("unknown",inplace=True)
test_data.keyword.fillna("none",inplace=True)

train_data
'''

# training documents with extracted columns 
train_data

#storing target values
Y=train_data['target']

#as categorical data is not taken by linear regression so er are dropping columns:"location","keyword","text"
train_data.drop(train_data.columns[[0,1,2]], axis=1, inplace=True)
train_data

from sklearn.model_selection import train_test_split
#splitting train.csv into 80:20 ration as train and test data
xtrain_new , xtest_new , y_train , y_test = train_test_split(train_data,Y,test_size=0.2)

# 80% training data
xtrain_new

#reseting index for clearity and sequentiality
xtrain_new.reset_index(drop=True, inplace=True) 
xtrain_new

#resetting xtest and ytest,ytrain index
xtest_new.reset_index(drop=True, inplace=True) 
y_train.reset_index(drop=True, inplace=True) 
y_test.reset_index(drop=True, inplace=True)

#linear regression function which calls hypo and bgd fucntion for computing regression coefficients and error values
def LR(x, y, a, num):
    n = x.shape[1]
    temp = np.ones((len(x),1))
    x = np.concatenate((temp, x), axis = 1)
    coef = np.zeros(n+1)
    hypo = hypothesis(coef, x, n)
   
    coef, error = BGD(coef,a,num,hypo,x,y,n)
    return coef, error

#this function computes error (distance from line which classifies documents) 
def BGD(coef, a, num, hypo, x, y, n):
    error = np.ones(num)
    for i in tqdm(range(num)):
        
        #optimizing regression coefficients
        coef[0] = coef[0] - (a/len(x)) * sum(hypo - y)
        for j in range(1,n+1):
            coef[j] = coef[j] - (a/len(x)) *sum((hypo-y) * x.transpose()[j])
        hypo = hypothesis(coef, x, n)
        
        #computing error
        error[i] = (1/len(x)) * 0.5 * sum((hypo-y)*(hypo - y))
    coef = coef.reshape(1,n+1)
    return coef,error

#this function perform matrix multiplication between regression coefficients and training tfidf values
def hypothesis(coef, x, n):
    coef = coef.reshape(1,n+1)
    hypo = np.ones((len(x),1))
    for i in range(len(x)):
        hypo[i] = float(np.matmul(coef, x[i]))
    hypo = hypo.reshape(x.shape[0])
    return hypo

from tqdm import tqdm

#training data
xtrain_new

type(xtrain_new)

#scaling tfidf doc term matrix for faster computation

mean = np.ones(xtrain_new.shape[1])
std = np.ones(xtrain_new.shape[1])
print(mean)
Xtrain_new=xtrain_new.to_numpy()
col=xtrain_new.shape[1]
for i in tqdm(range(col)):
    mean[i] = np.mean(xtrain_new.transpose()[i])
    std[i] = np.std(xtrain_new.transpose()[i])
    
    for j in range(len(x_train_new)):
        Xtrain_new[j][i] = (Xtrain_new[j][i] - mean[i])/std[i]

#calling linear regression function      
coef, error = tqdm(LR(Xtrain_new, y_train,0.0001, 3000))
#https://towardsdatascience.com/implementation-of-multi-variate-linear-regression-in-python-using-gradient-descent-optimization-b02f386425b9

xtest_new.shape[1]

# predicting target value for 20% data
xtest_new = np.concatenate((np.ones((xtest_new.shape[0],1)), xtest_new),axis = 1)

predictions = hypo(coef, xtest_new, xtest_new.shape[1] - 1)

#we get a single array of predictions where a threshold is needed to classify data we are taking 0.5 as threshold 
#values in prediction array >0.5 are of class 1 and below are class 0

for i in range(0, pred.shape[0]):
    if pred[i] > 0.5:
        pred[i] = 1
    else:
        pred[i] = 0
k = 0
for i in range(0, predictions.shape[0]):
    if predictions[i] == y_test[i]:
        k = k + 1
accuracy = k/y_test.shape[0]

accuracy



