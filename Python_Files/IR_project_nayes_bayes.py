# -*- coding: utf-8 -*-
"""Nayes_Bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NMJIYE3IVEXXOTzHkqxN-KKbEXz58S2f

**NAIVE-BAYES**
"""

import math
import pandas as pd
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
from collections import defaultdict
from sklearn.metrics import accuracy_score
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
from nltk.corpus import stopwords
import string
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""Reading train and test data"""

traindata = pd.read_csv('/content/drive/My Drive/train.csv')
testdata = pd.read_csv('/content/drive/My Drive/test.csv')
print(traindata.columns)
print(testdata.columns)
ids = list(traindata['id'])
target_train=list(traindata['target'])
#tweet_text1=list(testdata['text'])
original = []
computed = []
vocab=[]
real = defaultdict(list)
not_real = defaultdict(list)
class_dictionary=[real,not_real]
real_count=0
not_real_count=0
class_count=[real_count,not_real_count]
orig_class_train = [target_train]
ckk = [0]
vocab=[]

"""Function to find the maximum in an array"""

def find_max(arr):
    maxi = arr[0]
    for i in range(len(arr)):
        if(arr[i]>maxi):
            maxi = arr[i]
    return(arr.index(maxi))

"""Calculate the prior probability"""

for i in range(5000):
    tokens = nltk.word_tokenize(traindata['text'][i])
    tokens = [w.lower() for w in tokens]
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    words = [word for word in stripped if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    words = [w for w in words if not w in stop_words]
    lmt = [wordnet_lemmatizer.lemmatize(word) for word in words]
    lm = [ps.stem(word) for word in lmt]
    if(traindata['target'][i]==1):
        real_count=real_count+len(lm)
    else:
        not_real_count=not_real_count+len(lm)
    for l in lm:
        if(l not in vocab):
            vocab.append(l)
        if(traindata['target'][i] == 1):
            value = real.get(l,"Empty")
            if(value == "Empty"):
                real[l].append(1)
            else:
                v = value[0]+1
                real[l].pop()
                real[l].append(v)
        else:
            value = not_real.get(l,"Empty")
            if(value == "Empty"):
                not_real[l].append(1)
            else:
                v = value[0]+1
                not_real[l].pop()
                not_real[l].append(v)

"""Predicting label for test data which is obtained after train-test split"""

for i in range(5001,len(traindata['target'])):
    original.append(traindata['target'][i])
    tokens = nltk.word_tokenize(traindata['text'][i])
    tokens = [w.lower() for w in tokens]
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    words = [word for word in stripped if word.isalpha()]
    stop_words = set(stopwords.words('english'))
    words = [w for w in words if not w in stop_words]
    lmt = [wordnet_lemmatizer.lemmatize(word) for word in words]
    lm = [ps.stem(word) for word in lmt]
    pro = 0
    pro1 = 0
    for l in lm:
        value=real.get(l,"Empty")
        if(value == "Empty"):
            v=0
        else:
            v = value[0]
        pro=pro + math.log10((1+v)/(real_count+len(vocab)))
        value1 = not_real.get(l,"Empty")
        if(value1 == "Empty"):
            v1=0
                
        else:
            v1 = value1[0]
        pro1=pro1 + math.log10((1+v1)/(not_real_count+len(vocab)))
    if(pro1>pro):
        computed.append(0)
    else:
        computed.append(1)

print("Accuracy of Naive-Bayes model is : ",accuracy_score(original,computed))